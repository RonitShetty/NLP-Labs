{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1fcpqUOjBzhgt2b4dinkr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonitShetty/NLP-Labs/blob/main/C070_RonitShetty_NLPLab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Lab 8\n",
        "****\n",
        "**Aim:** Explore various Word Embedding Techniques/ Vector space models\n",
        "\n",
        "a)\tImplementation of Word2Vec word embedding technique to observe similarity between two words/sentences\n",
        "\n",
        "b)\timplementation of GloVe word embedding technique to measure semantic similarity\n",
        "\n",
        "\n",
        "**Roll No.:** C070  \n",
        "**Name:** Ronit Shetty  \n",
        "**SAP ID:** 70322000128  \n",
        "**Division:** C  \n",
        "**Batch:** C1  "
      ],
      "metadata": {
        "id": "3_aWjYgtyCiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell handles all the necessary installations, imports, and data downloads.\n",
        "\n",
        "# Install gensim for word embedding models\n",
        "!pip install -q gensim\n",
        "\n",
        "# Install nltk for text processing\n",
        "!pip install -q nltk\n",
        "\n",
        "print(\"Libraries installed.\")\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import string # To help remove punctuation\n",
        "import warnings\n",
        "\n",
        "# Suppress deprecation warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# --- Download NLTK Resources ---\n",
        "# 'punkt' is for tokenization.\n",
        "# 'stopwords' is for the list of common English stop words.\n",
        "print(\"\\nDownloading NLTK resources...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "print(\"NLTK resources downloaded.\")\n",
        "\n",
        "print(\"\\nEnvironment is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJNHXqswz0y2",
        "outputId": "33b0d4b1-8f0a-42b2-a9ce-1f35d248d440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed.\n",
            "\n",
            "Downloading NLTK resources...\n",
            "NLTK resources downloaded.\n",
            "\n",
            "Environment is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load a pre-trained Word2Vec model ---\n",
        "# We are using Google's powerful model trained on a massive Google News dataset.\n",
        "# It includes 300-dimensional vectors for 3 million words and phrases.\n",
        "# Note: This download is ~1.6GB and might take a few minutes.\n",
        "print(\"Downloading the 'word2vec-google-news-300' model. This may take a moment...\")\n",
        "try:\n",
        "    word2vec_model = api.load('word2vec-google-news-300')\n",
        "    print(\"\\nWord2Vec Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# --- 2. Exploring Word2Vec Capabilities ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PART A: EXPLORING WORD2VEC WORD RELATIONSHIPS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Example 1: Finding the most similar words\n",
        "print(\"\\nWords most similar to 'programming':\")\n",
        "print(word2vec_model.most_similar('programming', topn=5))\n",
        "\n",
        "# Example 2: Famous Analogies (Vector Arithmetic)\n",
        "# This demonstrates that the model captures complex semantic relationships.\n",
        "print(\"\\nTesting analogy: 'king' - 'man' + 'woman' approx ?\")\n",
        "print(word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
        "\n",
        "print(\"\\nTesting analogy: 'Paris' - 'France' + 'Germany' approx ?\")\n",
        "print(word2vec_model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=1))\n",
        "\n",
        "\n",
        "# Example 3: Measuring similarity between word pairs\n",
        "print(\"\\nMeasuring similarity scores between pairs:\")\n",
        "pairs = [('cat', 'dog'), ('car', 'vehicle'), ('computer', 'keyboard'), ('banana', 'rocket')]\n",
        "for w1, w2 in pairs:\n",
        "    score = word2vec_model.similarity(w1, w2)\n",
        "    print(f\"Similarity between '{w1}' and '{w2}': {score:.4f}\")\n",
        "\n",
        "# Example 4: Finding the odd one out\n",
        "print(\"\\nWhich word doesn't match?\")\n",
        "word_list = ['breakfast', 'cereal', 'dinner', 'lunch', 'computer']\n",
        "odd_one_out = word2vec_model.doesnt_match(word_list)\n",
        "print(f\"In the list {word_list}, the odd one out is: '{odd_one_out}'\")\n",
        "\n",
        "\n",
        "# --- 3. Find Similarity Between Two Documents ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PART B: CALCULATING DOCUMENT SIMILARITY WITH WORD2VEC\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define document pairs for comparison\n",
        "doc_pairs = [\n",
        "    {\n",
        "        \"title\": \"Similar Pair (AI/ML)\",\n",
        "        \"doc1\": \"Natural Language Processing is a fascinating field of artificial intelligence.\",\n",
        "        \"doc2\": \"AI and machine learning have revolutionized the way we interact with technology.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Dissimilar Pair (Tech vs. Nature)\",\n",
        "        \"doc1\": \"The new GPU offers amazing performance for deep learning tasks.\",\n",
        "        \"doc2\": \"The river flows gently through the lush green valley.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Define reusable text processing and vectorization functions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Cleans and tokenizes a text string.\"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    # Filter out punctuation and stop words\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "def get_document_vector(doc_tokens, model):\n",
        "    \"\"\"Averages word vectors to create a single document vector.\"\"\"\n",
        "    # Keep only words that are in the model's vocabulary\n",
        "    word_vectors = [model[word] for word in doc_tokens if word in model.key_to_index]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Process and compare each pair of documents\n",
        "for pair in doc_pairs:\n",
        "    print(f\"\\n--- Comparing: {pair['title']} ---\")\n",
        "    doc1, doc2 = pair['doc1'], pair['doc2']\n",
        "    print(f\"Doc 1: '{doc1}'\")\n",
        "    print(f\"Doc 2: '{doc2}'\")\n",
        "\n",
        "    # Pre-process the documents\n",
        "    tokens1 = preprocess(doc1)\n",
        "    tokens2 = preprocess(doc2)\n",
        "\n",
        "    # Get the vector representation for each document\n",
        "    vector1 = get_document_vector(tokens1, word2vec_model)\n",
        "    vector2 = get_document_vector(tokens2, word2vec_model)\n",
        "\n",
        "    # Calculate and print the Cosine Similarity\n",
        "    cosine_similarity = 1 - spatial.distance.cosine(vector1, vector2)\n",
        "    print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CarM7sIS08QH",
        "outputId": "e456733b-dbd7-4e2c-ba4f-15116f923dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the 'word2vec-google-news-300' model. This may take a moment...\n",
            "\n",
            "Word2Vec Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PART A: EXPLORING WORD2VEC WORD RELATIONSHIPS\n",
            "==================================================\n",
            "\n",
            "Words most similar to 'programming':\n",
            "[('programing', 0.8606171011924744), ('Programming', 0.6899746060371399), ('NLP_neuro_linguistic', 0.6174069046974182), ('broadcasts', 0.5984179377555847), ('primetime_programming', 0.5968459248542786)]\n",
            "\n",
            "Testing analogy: 'king' - 'man' + 'woman' approx ?\n",
            "[('queen', 0.7118193507194519)]\n",
            "\n",
            "Testing analogy: 'Paris' - 'France' + 'Germany' approx ?\n",
            "[('Berlin', 0.7644002437591553)]\n",
            "\n",
            "Measuring similarity scores between pairs:\n",
            "Similarity between 'cat' and 'dog': 0.7609\n",
            "Similarity between 'car' and 'vehicle': 0.7821\n",
            "Similarity between 'computer' and 'keyboard': 0.3964\n",
            "Similarity between 'banana' and 'rocket': 0.0650\n",
            "\n",
            "Which word doesn't match?\n",
            "In the list ['breakfast', 'cereal', 'dinner', 'lunch', 'computer'], the odd one out is: 'computer'\n",
            "\n",
            "==================================================\n",
            "PART B: CALCULATING DOCUMENT SIMILARITY WITH WORD2VEC\n",
            "==================================================\n",
            "\n",
            "--- Comparing: Similar Pair (AI/ML) ---\n",
            "Doc 1: 'Natural Language Processing is a fascinating field of artificial intelligence.'\n",
            "Doc 2: 'AI and machine learning have revolutionized the way we interact with technology.'\n",
            "Cosine Similarity: 0.4688\n",
            "\n",
            "--- Comparing: Dissimilar Pair (Tech vs. Nature) ---\n",
            "Doc 1: 'The new GPU offers amazing performance for deep learning tasks.'\n",
            "Doc 2: 'The river flows gently through the lush green valley.'\n",
            "Cosine Similarity: 0.2861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load a pre-trained GloVe model ---\n",
        "# We'll use a model trained on Wikipedia. It's smaller and faster to load.\n",
        "# 'glove-wiki-gigaword-100' means it uses a 100-dimensional vector for each word.\n",
        "print(\"Downloading the 'glove-wiki-gigaword-100' model...\")\n",
        "try:\n",
        "    glove_model = api.load('glove-wiki-gigaword-100')\n",
        "    print(\"\\nGloVe Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# --- 2. Exploring GloVe Capabilities ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PART A: EXPLORING GLOVE WORD RELATIONSHIPS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Example 1: Finding the most similar words\n",
        "print(\"\\nWords most similar to 'technology':\")\n",
        "print(glove_model.most_similar('technology', topn=5))\n",
        "\n",
        "# Example 2: Famous Analogies (Vector Arithmetic)\n",
        "print(\"\\nTesting analogy: 'king' - 'man' + 'woman' approx ?\")\n",
        "print(glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
        "\n",
        "print(\"\\nTesting analogy: 'rome' - 'italy' + 'japan' approx ?\")\n",
        "print(glove_model.most_similar(positive=['rome', 'japan'], negative=['italy'], topn=1))\n",
        "\n",
        "# Example 3: Measuring similarity between word pairs\n",
        "print(\"\\nMeasuring similarity scores between pairs:\")\n",
        "pairs = [('cat', 'dog'), ('car', 'vehicle'), ('computer', 'keyboard'), ('banana', 'rocket')]\n",
        "for w1, w2 in pairs:\n",
        "    score = glove_model.similarity(w1, w2)\n",
        "    print(f\"Similarity between '{w1}' and '{w2}': {score:.4f}\")\n",
        "\n",
        "# Example 4: Finding the odd one out\n",
        "print(\"\\nWhich word doesn't match?\")\n",
        "word_list = ['apple', 'banana', 'orange', 'fruit', 'bicycle']\n",
        "odd_one_out = glove_model.doesnt_match(word_list)\n",
        "print(f\"In the list {word_list}, the odd one out is: '{odd_one_out}'\")\n",
        "\n",
        "\n",
        "# --- 3. Find Similarity Between Two Documents using GloVe ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PART B: CALCULATING DOCUMENT SIMILARITY WITH GLOVE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# We will reuse the same document pairs from the Word2Vec section\n",
        "# and the same helper functions (preprocess, get_document_vector)\n",
        "\n",
        "doc_pairs = [\n",
        "    {\n",
        "        \"title\": \"Similar Pair (AI/ML)\",\n",
        "        \"doc1\": \"Natural Language Processing is a fascinating field of artificial intelligence.\",\n",
        "        \"doc2\": \"AI and machine learning have revolutionized the way we interact with technology.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Dissimilar Pair (Tech vs. Nature)\",\n",
        "        \"doc1\": \"The new GPU offers amazing performance for deep learning tasks.\",\n",
        "        \"doc2\": \"The river flows gently through the lush green valley.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Process and compare each pair of documents using the GloVe model\n",
        "for pair in doc_pairs:\n",
        "    print(f\"\\n--- Comparing: {pair['title']} ---\")\n",
        "    doc1, doc2 = pair['doc1'], pair['doc2']\n",
        "    print(f\"Doc 1: '{doc1}'\")\n",
        "    print(f\"Doc 2: '{doc2}'\")\n",
        "\n",
        "    # Pre-process the documents\n",
        "    tokens1 = preprocess(doc1)\n",
        "    tokens2 = preprocess(doc2)\n",
        "\n",
        "    # Get the vector representation for each document using the GLOVE model\n",
        "    vector1 = get_document_vector(tokens1, glove_model)\n",
        "    vector2 = get_document_vector(tokens2, glove_model)\n",
        "\n",
        "    # Calculate and print the Cosine Similarity\n",
        "    cosine_similarity = 1 - spatial.distance.cosine(vector1, vector2)\n",
        "    print(f\"Cosine Similarity (using GloVe): {cosine_similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z88MPUlQ1YTV",
        "outputId": "aecbaf10-f938-4ffb-cf4b-f4f5f0007958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the 'glove-wiki-gigaword-100' model...\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "\n",
            "GloVe Model loaded successfully!\n",
            "\n",
            "==================================================\n",
            "PART A: EXPLORING GLOVE WORD RELATIONSHIPS\n",
            "==================================================\n",
            "\n",
            "Words most similar to 'technology':\n",
            "[('technologies', 0.8506267666816711), ('computer', 0.7642159461975098), ('tech', 0.7489413619041443), ('software', 0.7358859181404114), ('systems', 0.7292639017105103)]\n",
            "\n",
            "Testing analogy: 'king' - 'man' + 'woman' approx ?\n",
            "[('queen', 0.7698540687561035)]\n",
            "\n",
            "Testing analogy: 'rome' - 'italy' + 'japan' approx ?\n",
            "[('tokyo', 0.7762303948402405)]\n",
            "\n",
            "Measuring similarity scores between pairs:\n",
            "Similarity between 'cat' and 'dog': 0.8798\n",
            "Similarity between 'car' and 'vehicle': 0.8631\n",
            "Similarity between 'computer' and 'keyboard': 0.5418\n",
            "Similarity between 'banana' and 'rocket': -0.0064\n",
            "\n",
            "Which word doesn't match?\n",
            "In the list ['apple', 'banana', 'orange', 'fruit', 'bicycle'], the odd one out is: 'bicycle'\n",
            "\n",
            "==================================================\n",
            "PART B: CALCULATING DOCUMENT SIMILARITY WITH GLOVE\n",
            "==================================================\n",
            "\n",
            "--- Comparing: Similar Pair (AI/ML) ---\n",
            "Doc 1: 'Natural Language Processing is a fascinating field of artificial intelligence.'\n",
            "Doc 2: 'AI and machine learning have revolutionized the way we interact with technology.'\n",
            "Cosine Similarity (using GloVe): 0.7458\n",
            "\n",
            "--- Comparing: Dissimilar Pair (Tech vs. Nature) ---\n",
            "Doc 1: 'The new GPU offers amazing performance for deep learning tasks.'\n",
            "Doc 2: 'The river flows gently through the lush green valley.'\n",
            "Cosine Similarity (using GloVe): 0.4764\n"
          ]
        }
      ]
    }
  ]
}