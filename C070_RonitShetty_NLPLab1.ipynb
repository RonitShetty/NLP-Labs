{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonitShetty/NLP-Labs/blob/main/C070_RonitShetty_NLPLab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Lab 1  \n",
        "**Roll No.:** C070  \n",
        "**Name:** Ronit Shetty  \n",
        "**SAP ID:** 70322000128  \n",
        "**Division:** C  \n",
        "**Batch:** C1  \n"
      ],
      "metadata": {
        "id": "hAFvaszDWIYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP - Text Processing\n",
        "\n",
        " **Preprocessing of Text Data**:\n",
        "Preprocessing is the first and crucial step in any NLP pipeline. It involves cleaning and structuring raw text data to make it suitable for further analysis or model input.\n",
        "\n",
        "**Regular Expressions (re module)**:\n",
        "\n",
        "\n",
        "* Remove URLs, punctuation, digits, etc.\n",
        "* Convert text to lowercase\n",
        "\n",
        "\n",
        "Sample raw text:\n",
        "\n",
        "text = \"Visit us at https://example.com! NLP is 100% FUN.\"\n",
        "\n",
        "Preprocessing using regex:\n",
        "\n",
        "cleaned_text = re.sub(r\"http\\S+|[^a-zA-Z\\s]\", \"\", text.lower())\n",
        "\n",
        "print(cleaned_text)  # Output: \"visit us at  nlp is  fun\"\n",
        "\n",
        "**Tokenization**:\n",
        "\n",
        "Word Tokenization: Break text into words\n",
        "→ \"I ate food\" → [\"I\", \"ate\", \"food\"]\n",
        "\n",
        "Sentence Tokenization: Split paragraph into sentences\n",
        "→ \"I ate food. What did I eat?\" → [\"I ate food.\", \"What did I eat?\"]"
      ],
      "metadata": {
        "id": "sxNcXP2uVfSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation\n",
        "\n",
        "Tokenization is the process of breaking text into smaller units called tokens. These tokens can be words, subwords, or sentences, depending on the task at hand. It helps machines understand and work with text efficiently.\n",
        "\n",
        "Depending on your problem, you can choose between:\n",
        "\n",
        "* Sentence Tokenization: Splits text into complete sentences.\n",
        "* Word Tokenization: Splits text into individual words or punctuation marks.\n",
        "\n",
        "To perform tokenization easily, we use the NLTK (Natural Language Toolkit) library.\n",
        "\n",
        "install nltk package\n",
        "\n",
        "import nltk"
      ],
      "metadata": {
        "id": "saIITLS9V_-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySAWS_3UVVSQ",
        "outputId": "168db6e7-e32a-4592-fc9e-bfd124fea9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab') # punkt - word_tokenize and sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrfvWwm3WGoZ",
        "outputId": "2164a6ba-9443-4259-8eb2-73fe3fc8df60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence and word tokenization"
      ],
      "metadata": {
        "id": "p3DmNSOanZrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I am Ronit Shetty, a passionate computer engineer currently pursuing my B.Tech degree at NMIMS University. With a deep love for coding and a forward-looking vision, I aspire to become a skilled AI/ML engineer. My academic journey is fueled by curiosity, a drive to innovate, and a relentless desire to explore how intelligent systems can solve real-world problems.\n",
        "\n",
        "I actively work on projects ranging from mobile app development to reinforcement learning simulations and Chrome extensions that integrate AI directly into the browser. Whether it's building an AI-powered food scanner, crafting a multiplayer price-guessing game, or fine-tuning reinforcement learning agents, I enjoy turning ideas into working systems.\n",
        "\n",
        "Apart from technical skills, I value clear communication, structured thinking, and continuous learning. My work spans Python, Java, React, SQL, and more — always with an eye on the future of AI and intelligent automation.\n",
        "\n",
        "My ultimate goal? To build tools that are not just smart, but also useful, impactful, and deeply human-centric.\"\"\"\n"
      ],
      "metadata": {
        "id": "g3Jah2XZW3EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# removing digits\n",
        "paragraph= re.sub(r'\\d+', ' ', paragraph)\n",
        "print(paragraph)\n",
        "\n",
        "# removing punctuation\n",
        "import string\n",
        "paragraph = paragraph.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVVdtQ0UnefC",
        "outputId": "5853f17a-2e76-4098-89b0-29d2eadf56fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Ronit Shetty, a passionate computer engineer currently pursuing my B.Tech degree at NMIMS University. With a deep love for coding and a forward-looking vision, I aspire to become a skilled AI/ML engineer. My academic journey is fueled by curiosity, a drive to innovate, and a relentless desire to explore how intelligent systems can solve real-world problems.\n",
            "\n",
            "I actively work on projects ranging from mobile app development to reinforcement learning simulations and Chrome extensions that integrate AI directly into the browser. Whether it's building an AI-powered food scanner, crafting a multiplayer price-guessing game, or fine-tuning reinforcement learning agents, I enjoy turning ideas into working systems.\n",
            "\n",
            "Apart from technical skills, I value clear communication, structured thinking, and continuous learning. My work spans Python, Java, React, SQL, and more — always with an eye on the future of AI and intelligent automation.\n",
            "\n",
            "My ultimate goal? To build tools that are not just smart, but also useful, impactful, and deeply human-centric.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSdQq4_OXW0g",
        "outputId": "cc370520-e710-4c29-9db4-d30a10319475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am Ronit Shetty a passionate computer engineer currently pursuing my BTech degree at NMIMS University With a deep love for coding and a forwardlooking vision I aspire to become a skilled AIML engineer My academic journey is fueled by curiosity a drive to innovate and a relentless desire to explore how intelligent systems can solve realworld problems\\n\\nI actively work on projects ranging from mobile app development to reinforcement learning simulations and Chrome extensions that integrate AI directly into the browser Whether its building an AIpowered food scanner crafting a multiplayer priceguessing game or finetuning reinforcement learning agents I enjoy turning ideas into working systems\\n\\nApart from technical skills I value clear communication structured thinking and continuous learning My work spans Python Java React SQL and more — always with an eye on the future of AI and intelligent automation\\n\\nMy ultimate goal To build tools that are not just smart but also useful impactful and deeply humancentric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17WDGa4LXe-V",
        "outputId": "78c9803e-62e8-4c25-9e2a-6bab3553d7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'Ronit', 'Shetty', 'a', 'passionate', 'computer', 'engineer', 'currently', 'pursuing', 'my', 'BTech', 'degree', 'at', 'NMIMS', 'University', 'With', 'a', 'deep', 'love', 'for', 'coding', 'and', 'a', 'forwardlooking', 'vision', 'I', 'aspire', 'to', 'become', 'a', 'skilled', 'AIML', 'engineer', 'My', 'academic', 'journey', 'is', 'fueled', 'by', 'curiosity', 'a', 'drive', 'to', 'innovate', 'and', 'a', 'relentless', 'desire', 'to', 'explore', 'how', 'intelligent', 'systems', 'can', 'solve', 'realworld', 'problems', 'I', 'actively', 'work', 'on', 'projects', 'ranging', 'from', 'mobile', 'app', 'development', 'to', 'reinforcement', 'learning', 'simulations', 'and', 'Chrome', 'extensions', 'that', 'integrate', 'AI', 'directly', 'into', 'the', 'browser', 'Whether', 'its', 'building', 'an', 'AIpowered', 'food', 'scanner', 'crafting', 'a', 'multiplayer', 'priceguessing', 'game', 'or', 'finetuning', 'reinforcement', 'learning', 'agents', 'I', 'enjoy', 'turning', 'ideas', 'into', 'working', 'systems', 'Apart', 'from', 'technical', 'skills', 'I', 'value', 'clear', 'communication', 'structured', 'thinking', 'and', 'continuous', 'learning', 'My', 'work', 'spans', 'Python', 'Java', 'React', 'SQL', 'and', 'more', '—', 'always', 'with', 'an', 'eye', 'on', 'the', 'future', 'of', 'AI', 'and', 'intelligent', 'automation', 'My', 'ultimate', 'goal', 'To', 'build', 'tools', 'that', 'are', 'not', 'just', 'smart', 'but', 'also', 'useful', 'impactful', 'and', 'deeply', 'humancentric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords\n",
        "\n",
        "Stopwords are common words in a language that are often removed during text preprocessing because they carry little to no meaningful information for tasks like text classification, sentiment analysis, or topic modeling.\n",
        "\n",
        "These include words like \"the\", \"is\", \"in\", \"and\", etc., which are frequently used but rarely contribute significant insight into the overall meaning of the text.\n",
        "\n",
        "Why Remove Stopwords?\n",
        "\n",
        "They do not help in understanding the core content.\n",
        "They inflate the size of the data unnecessarily.\n",
        "Removing them helps in improving model efficiency and reducing noise.\n",
        "\n",
        "\n",
        "Sample Stopwords from NLTK:\n",
        "\n",
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
        " 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
        " \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
        " 'he', 'she', 'it', 'they', 'them', 'his', 'her',\n",
        " 'was', 'were', 'am', 'is', 'are', 'be', 'been',\n",
        " 'being', 'have', 'has', 'had', 'having', 'do',\n",
        " 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
        " 'but', 'if', 'or', 'because', 'as', 'until', 'while']\n"
      ],
      "metadata": {
        "id": "8KGdAQ8DkKsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYDcGoUAX3t2",
        "outputId": "b2dc67f2-8688-4b87-9fe1-11bec1493ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "5TIh8ovVZOji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bGp7QfYq_3",
        "outputId": "f66e6ac7-50a3-4ad3-fc5e-ec2b56521d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming\n",
        "Stemming is a type of text normalization where words are reduced to their root or base form. This helps simplify variations of a word so they can be analyzed as one.\n",
        "\n",
        "What It Does:\n",
        "It chops off prefixes or suffixes to reduce related words to a common base.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"running\", \"runner\", \"ran\" → \"run\"\n",
        "\n",
        "Limitation:\n",
        "While stemming helps in reducing word complexity, it's a crude process and doesn't always produce real English words. Sometimes the root form may look unnatural or lose its original meaning.\n",
        "\n",
        "For instance:\n",
        "\n",
        "\"universities\" → \"univers\"\n",
        "\n",
        "\"relational\" → \"relat\"\n",
        "\n",
        "This can affect the quality of interpretation in some NLP tasks."
      ],
      "metadata": {
        "id": "TbYPpcAHkVdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "8mE9jGlXZnG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words =  [\"running\", \"runner\", \"ran\", \"run\"]"
      ],
      "metadata": {
        "id": "y5I_8c2ObBhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vnB4K8Z7Hn",
        "outputId": "a73f0261-8f75-41ea-cc59-49ed8f12ad6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'ran', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming vs Lemmatization\n",
        "**Stemming**:\n",
        "Think of stemming as a brute-force approach. It cuts off word endings using predefined rules, often without caring if the result is an actual word.\n",
        "\n",
        "Speed: Very fast\n",
        "\n",
        "Accuracy: Not always reliable\n",
        "\n",
        "Output: Can be non-English or incomplete forms\n",
        "\n",
        "Examples:\n",
        "\n",
        "\"caring\" → \"car\"\n",
        "\n",
        "\"fishing\" → \"fish\"\n",
        "\n",
        "\"relational\" → \"relat\"\n",
        "\n",
        "Not all stems are proper words. It's fast, but rough around the edges.\n",
        "\n",
        "**Lemmatization**:\n",
        "Lemmatization is smarter. It uses dictionaries and grammar rules to return the proper base form (called a lemma). It considers things like part of speech and context.\n",
        "\n",
        "Speed: Slower\n",
        "\n",
        "Accuracy: High and linguistically correct\n",
        "\n",
        "Output: Real words\n",
        "\n",
        "Examples:\n",
        "\n",
        "\"caring\" → \"care\"\n",
        "\n",
        "\"fishing\" → \"fish\"\n",
        "\n",
        "\"better\" → \"good\"\n",
        "\n",
        "\"went\" → \"go\"\n",
        "\n",
        "More thoughtful, more accurate — but also a bit heavier computationally.\n",
        "\n"
      ],
      "metadata": {
        "id": "zFIYr5hAbGEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization\n",
        "Lemmatization is the process of reducing a word to its base or dictionary form — called a lemma — without losing its meaning.\n",
        "\n",
        "Unlike stemming, which blindly chops off word endings, lemmatization uses a vocabulary and grammar-aware approach. It ensures that the resulting word is both meaningful and grammatically correct.\n",
        "\n",
        "How it Works:\n",
        "It looks up each word in a predefined dictionary (like WordNet).\n",
        "\n",
        "Considers the part of speech (POS) to find the correct base form.\n",
        "\n",
        "Returns a valid English word that can be used in context.\n",
        "\n",
        "Examples:\n",
        "\n",
        "\"am\", \"are\", \"is\" → \"be\"  \n",
        "\"better\" → \"good\"  \n",
        "\"studies\" → \"study\"  \n",
        "\"running\" → \"run\"  \n",
        "\"cats\" → \"cat\"\n"
      ],
      "metadata": {
        "id": "F64JDra8brAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1fusB9farEI",
        "outputId": "ae151f9e-ef84-4bc5-bd86-6b075c1f187b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "MOGfF26UbzPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"flying\") # Since no part-of-speech is provided, it treats \"flying\" as a noun — so it doesn’t change."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SMjRQCKdb6Wf",
        "outputId": "0e105f8b-0c9d-4465-8095-8a49f6625db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'flying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"flying\", pos='v') # pos(Part of speech) = aditional information about the word\n",
        "# v - verb , a - adjective, n-noun, default\n",
        "# With pos='v' (verb), it correctly reduces \"flying\" to its base form \"fly\"."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fs3JUQmNcDps",
        "outputId": "713afaf5-cac3-46eb-a1b7-3eb671e13de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=\"a\") # \"Better\" is the comparative form of the adjective \"good\", and lemmatization knows this!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mm72OUaZn5ke",
        "outputId": "3424040f-20bb-47c5-9113-50f3b8049642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"Built\", \"Building\", \"Builds\", \"Build\"]"
      ],
      "metadata": {
        "id": "C3x-ZiGYcRdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM5-UqHIdDLT",
        "outputId": "5941631a-43ea-4a62-8aa9-a49dd51d0576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Built', 'Building', 'Builds', 'Build']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have to change it to lower case for it to work\n",
        "words = [lemmatizer.lemmatize(word.lower(), pos='v') for word in words]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO2BQd_WdNrv",
        "outputId": "b461a486-b432-4f71-b392-be17f458697f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['build', 'build', 'build', 'build']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "  print(sentences[i])"
      ],
      "metadata": {
        "id": "s6DlidhLdpCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06ff6f2-1168-4bdd-e5a1-a1ec2d6031ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Ronit Shetty passionate computer engineer currently pursuing BTech degree NMIMS University With deep love coding forwardlooking vision I aspire become skilled AIML engineer My academic journey fueled curiosity drive innovate relentless desire explore intelligent system solve realworld problem I actively work project ranging mobile app development reinforcement learning simulation Chrome extension integrate AI directly browser Whether building AIpowered food scanner crafting multiplayer priceguessing game finetuning reinforcement learning agent I enjoy turning idea working system Apart technical skill I value clear communication structured thinking continuous learning My work span Python Java React SQL — always eye future AI intelligent automation My ultimate goal To build tool smart also useful impactful deeply humancentric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy – Object-Oriented NLP\n",
        "spaCy is a modern NLP library designed with object-oriented principles, offering powerful and efficient tools for processing large-scale text data. Unlike traditional libraries like NLTK, which often return basic data structures (like lists or tuples), spaCy wraps everything in intelligent objects such as Doc, Token, and Span. These objects come with built-in methods and attributes that make NLP tasks feel more intuitive and Pythonic.\n",
        "\n",
        "For example, once you've tokenized a sentence with spaCy, you can access each word as a Token object, allowing you to instantly check its part of speech, lemma, dependency relation, etc.—all in one line.\n",
        "\n"
      ],
      "metadata": {
        "id": "dxu6nr0Lertq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJZ_1mb2enf2",
        "outputId": "24d1e6b8-bfb4-4b18-ebb6-1c7f778c8c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgxougZIe06c",
        "outputId": "b792e973-665e-4d08-eaf0-80dcd7ba642c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# New input sentence with a variety of word forms\n",
        "sentence = \"running ran runs easily beautiful dancing confusion driven sings brightest darkness coded quietly sleeping\"\n",
        "\n",
        "# Process the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print each token, its POS (part-of-speech), and its lemma\n",
        "print(f\"{'Token':<12} | {'POS':<10} | Lemma\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} | {token.pos_:<10} | {token.lemma_}\")"
      ],
      "metadata": {
        "id": "dw1KF3n1e6Qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29c922b-5c7a-4873-f0b2-5912a439772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token        | POS        | Lemma\n",
            "----------------------------------------\n",
            "running      | VERB       | run\n",
            "ran          | VERB       | run\n",
            "runs         | VERB       | run\n",
            "easily       | ADV        | easily\n",
            "beautiful    | ADJ        | beautiful\n",
            "dancing      | NOUN       | dancing\n",
            "confusion    | NOUN       | confusion\n",
            "driven       | VERB       | drive\n",
            "sings        | NOUN       | sing\n",
            "brightest    | ADJ        | bright\n",
            "darkness     | NOUN       | darkness\n",
            "coded        | VERB       | code\n",
            "quietly      | ADV        | quietly\n",
            "sleeping     | VERB       | sleep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question of Curiosity:\n",
        "\n",
        "**1. What is the advantage of normalizing a word?**  \n",
        "**Ans:** Normalizing a word allows different forms of the same word to be treated as one, which improves the consistency and efficiency of text processing. For example, *\"run\"*, *\"running\"*, *\"runs\"*, and *\"ran\"* are all different forms of the same action. Without normalization, a machine learning model would treat them as unrelated, increasing complexity and noise. By reducing words to a standard form, normalization helps improve text matching, reduces feature space in models, and boosts performance in tasks like document classification, information retrieval, and sentiment analysis.\n",
        "\n",
        "**2. Difference between Stemming and Lemmatization.**  \n",
        "**Ans:** Stemming and lemmatization are both normalization techniques, but they differ in approach and accuracy.  \n",
        "- **Stemming** is a rule-based process that chops off word suffixes to reach a root form, often ignoring grammar. It is fast but can produce incorrect or non-existent words (e.g., *“studies” → “studi”*).  \n",
        "- **Lemmatization** uses dictionaries and morphological analysis to convert a word into its proper base or lemma, considering its part of speech. It is slower but more accurate (e.g., *“was” → “be”*, *“better” → “good”*).  \n",
        "\n",
        "**In short:** Stemming is crude and quick, while lemmatization is intelligent and precise.\n",
        "\n",
        "**3. Examples of some use cases where to use stemming and lemmatization.**  \n",
        "**Ans:**  \n",
        "- **Stemming** is preferred in scenarios where performance and speed are more critical than linguistic precision. For instance, in search engines or recommendation systems, where approximate matching is acceptable and computational speed is important, stemming reduces words quickly without much processing.  \n",
        "- **Lemmatization** is useful in applications requiring deeper language understanding and contextual accuracy, like chatbots, virtual assistants, sentiment analysis, and language translation systems. These tasks benefit from grammatically correct and meaningful base words, which improve model interpretability and user experience.\n"
      ],
      "metadata": {
        "id": "K_eTb4Q47e_M"
      }
    }
  ]
}