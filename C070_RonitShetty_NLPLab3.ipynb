{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4B371ZvroppQGZ3MCV37J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonitShetty/NLP-Labs/blob/main/C070_RonitShetty_NLPLab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gutenburg Corpus**\n",
        "\n",
        "The Gutenberg Corpus included in the Natural Language Toolkit (NLTK) is a small selection of texts from the much larger Project Gutenberg electronic text archive, which contains around 25,000 free electronic books. It is a simple collection of texts without any specific structure or categorization.\n",
        "\n",
        "\n",
        "\n",
        "Key features of the NLTK Gutenberg Corpus include:\n",
        "\n",
        "\n",
        "Content: It contains 18 texts, including works by authors like Jane Austen, William Blake, and Shakespeare, as well as the King James Version of the Bible.\n",
        "\n",
        "\n",
        "\n",
        "Access Methods: You can access the content as a raw string of text (raw()), a list of words (words()), or a list of sentences (sents()).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Usage: It is often used for initial explorations in Natural Language Processing, such as calculating word frequencies, finding concordance, and analyzing basic statistics like word and sentence length."
      ],
      "metadata": {
        "id": "Yg45NkNInytx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download the Gutenberg and Brown corpora, as well as the stopwords and words corpora used in the examples.\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "heotNH-4nzjw",
        "outputId": "d92dfa41-8c73-4288-eb88-d9f7c53587bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# List all file identifiers in the Gutenberg corpus\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Eqra15tqnWL2",
        "outputId": "61435af3-86fd-47dc-fbc9-5bf14a7a8671"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the words from 'austen-emma.txt'\n",
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "\n",
        "# Print the total number of words\n",
        "len(emma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tx9WDn3RnYsR",
        "outputId": "12f3ab20-04ba-4c6e-c802-cec5f1bfddea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192427"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the gutenberg corpus directly for easier access\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Loop through each fileid in the gutenberg corpus\n",
        "for fileid in gutenberg.fileids():\n",
        "  # Calculate statistics for each text\n",
        "  num_chars = len(gutenberg.raw(fileid))\n",
        "  num_words = len(gutenberg.words(fileid))\n",
        "  num_sents = len(gutenberg.sents(fileid))\n",
        "  # Calculate vocabulary size, converting words to lowercase\n",
        "  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "  # Print the rounded statistics for each file\n",
        "  print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7bK0ePBAna7D",
        "outputId": "7ae582fa-a833-4425-bc75-17658feaa05d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 25 26 austen-emma.txt\n",
            "5 26 17 austen-persuasion.txt\n",
            "5 28 22 austen-sense.txt\n",
            "4 34 79 bible-kjv.txt\n",
            "5 19 5 blake-poems.txt\n",
            "4 19 14 bryant-stories.txt\n",
            "4 18 12 burgess-busterbrown.txt\n",
            "4 20 13 carroll-alice.txt\n",
            "5 20 12 chesterton-ball.txt\n",
            "5 23 11 chesterton-brown.txt\n",
            "5 18 11 chesterton-thursday.txt\n",
            "4 21 25 edgeworth-parents.txt\n",
            "5 26 15 melville-moby_dick.txt\n",
            "5 52 11 milton-paradise.txt\n",
            "4 12 9 shakespeare-caesar.txt\n",
            "4 12 8 shakespeare-hamlet.txt\n",
            "4 12 7 shakespeare-macbeth.txt\n",
            "5 36 12 whitman-leaves.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw text\n",
        "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
        "print(\"Raw text sample:\", raw[1:20])\n",
        "\n",
        "# Word list\n",
        "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
        "print(\"\\nWords list sample:\", words[1:20])\n",
        "\n",
        "# Sentence list\n",
        "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
        "print(\"\\nSentences list sample:\", sents[1:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XrajQEY2ndwk",
        "outputId": "4f5775ce-7489-4073-b553-0c23b6bf0815"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text sample: The Adventures of B\n",
            "\n",
            "Words list sample: ['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.', 'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster', 'Bear']\n",
            "\n",
            "Sentences list sample: [['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as', 'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched', 'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', 'the', 'Green', 'Forest', 'to', 'chase', 'out', 'the', 'Black', 'Shadows', '.'], ['Once', 'more', 'he', 'yawned', ',', 'and', 'slowly', 'got', 'to', 'his', 'feet', 'and', 'shook', 'himself', '.'], ['Then', 'he', 'walked', 'over', 'to', 'a', 'big', 'pine', '-', 'tree', ',', 'stood', 'up', 'on', 'his', 'hind', 'legs', ',', 'reached', 'as', 'high', 'up', 'on', 'the', 'trunk', 'of', 'the', 'tree', 'as', 'he', 'could', ',', 'and', 'scratched', 'the', 'bark', 'with', 'his', 'great', 'claws', '.'], ['After', 'that', 'he', 'yawned', 'until', 'it', 'seemed', 'as', 'if', 'his', 'jaws', 'would', 'crack', ',', 'and', 'then', 'sat', 'down', 'to', 'think', 'what', 'he', 'wanted', 'for', 'breakfast', '.'], ['While', 'he', 'sat', 'there', ',', 'trying', 'to', 'make', 'up', 'his', 'mind', 'what', 'would', 'taste', 'best', ',', 'he', 'was', 'listening', 'to', 'the', 'sounds', 'that', 'told', 'of', 'the', 'waking', 'of', 'all', 'the', 'little', 'people', 'who', 'live', 'in', 'the', 'Green', 'Forest', '.'], ['He', 'heard', 'Sammy', 'Jay', 'way', 'off', 'in', 'the', 'distance', 'screaming', ',', '\"', 'Thief', '!'], ['Thief', '!\"'], ['and', 'grinned', '.'], ['\"', 'I', 'wonder', ',\"', 'thought', 'Buster', ',', '\"', 'if', 'some', 'one', 'has', 'stolen', 'Sammy', \"'\", 's', 'breakfast', ',', 'or', 'if', 'he', 'has', 'stolen', 'the', 'breakfast', 'of', 'some', 'one', 'else', '.'], ['Probably', 'he', 'is', 'the', 'thief', 'himself', '.\"'], ['He', 'heard', 'Chatterer', 'the', 'Red', 'Squirrel', 'scolding', 'as', 'fast', 'as', 'he', 'could', 'make', 'his', 'tongue', 'go', 'and', 'working', 'himself', 'into', 'a', 'terrible', 'rage', '.'], ['\"', 'Must', 'be', 'that', 'Chatterer', 'got', 'out', 'of', 'bed', 'the', 'wrong', 'way', 'this', 'morning', ',\"', 'thought', 'he', '.'], ['He', 'heard', 'Blacky', 'the', 'Crow', 'cawing', 'at', 'the', 'top', 'of', 'his', 'lungs', ',', 'and', 'he', 'knew', 'by', 'the', 'sound', 'that', 'Blacky', 'was', 'getting', 'into', 'mischief', 'of', 'some', 'kind', '.'], ['He', 'heard', 'the', 'sweet', 'voices', 'of', 'happy', 'little', 'singers', ',', 'and', 'they', 'were', 'good', 'to', 'hear', '.'], ['But', 'most', 'of', 'all', 'he', 'listened', 'to', 'a', 'merry', ',', 'low', ',', 'silvery', 'laugh', 'that', 'never', 'stopped', 'but', 'went', 'on', 'and', 'on', ',', 'until', 'he', 'just', 'felt', 'as', 'if', 'he', 'must', 'laugh', 'too', '.'], ['It', 'was', 'the', 'voice', 'of', 'the', 'Laughing', 'Brook', '.'], ['And', 'as', 'Buster', 'listened', 'it', 'suddenly', 'came', 'to', 'him', 'just', 'what', 'he', 'wanted', 'for', 'breakfast', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Brown Corpus**\n",
        "\n",
        "The Brown Corpus was a pioneering electronic corpus of English, created in 1961 at Brown University. It was the first million-word electronic corpus of the English language.\n",
        "\n",
        "Key features of the Brown Corpus include:\n",
        "\n",
        "\n",
        "Structure: The corpus is structured into 500 sources, which are categorized into 15 different genres, such as news, editorial, romance, and science fiction. This categorized structure makes it highly valuable for comparative linguistic studies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Content: It contains text from a wide variety of sources, balanced across the different genres.\n",
        "\n",
        "\n",
        "\n",
        "Usage: Its genre-based categorization makes it an excellent resource for stylistics, which is the study of systematic linguistic differences between genres. A common use case is to compare word usage (like modal verbs or \"wh\" words) across different categories to identify genre-specific patterns. You can also access its content by specific files or categories."
      ],
      "metadata": {
        "id": "IrYqQrLgoP2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "# List the categories in the Brown corpus\n",
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K_-0aP-ingI9",
        "outputId": "dca928ee-aeb5-4e8b-aa9d-2413d1c8a878"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get words from the 'news' genre\n",
        "news_words = brown.words(categories='news')\n",
        "print(\"First 10 words from the 'news' genre:\", news_words[:10])\n",
        "\n",
        "# Get sentences from the 'news', 'editorial', and 'reviews' genres\n",
        "multi_genre_sents = brown.sents(categories=['news', 'editorial', 'reviews'])\n",
        "print(\"\\nFirst 2 sentences from multiple genres:\", multi_genre_sents[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "M4Y52YZNniA6",
        "outputId": "ce424d42-05ac-45a6-be2e-3340e70c01b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words from the 'news' genre: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
            "\n",
            "First 2 sentences from multiple genres: [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "\n",
        "# Load words from the 'news' category\n",
        "news_text = brown.words(categories='news')\n",
        "# Create a frequency distribution of the lowercased words\n",
        "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
        "\n",
        "# List of modals to check\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# Print the frequency of each modal verb\n",
        "for m in modals:\n",
        "  print(m + ':', fdist[m], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bu-FeNvwnjWy",
        "outputId": "478c81bd-365b-4fc7-a581-7e9c13376be1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Create a conditional frequency distribution of words by genre.\n",
        "# This part of the code correctly includes all categories already.\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (genre, word)\n",
        "    for genre in brown.categories()\n",
        "    for word in brown.words(categories=genre))\n",
        "\n",
        "# Define the list of all genres directly from the corpus\n",
        "genres = brown.categories()\n",
        "\n",
        "# Define the modal verbs for comparison\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# Generate and print a table comparing the frequencies across all genres\n",
        "cfd.tabulate(conditions=genres, samples=modals)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gOVZG6cwnlt8",
        "outputId": "a165b7f4-c869-40fb-c6ba-aa609e4fe4bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  can could   may might  must  will \n",
            "      adventure    46   151     5    58    27    50 \n",
            " belles_lettres   246   213   207   113   170   236 \n",
            "      editorial   121    56    74    39    53   233 \n",
            "        fiction    37   166     8    44    55    52 \n",
            "     government   117    38   153    13   102   244 \n",
            "        hobbies   268    58   131    22    83   264 \n",
            "          humor    16    30     8     8     9    13 \n",
            "        learned   365   159   324   128   202   340 \n",
            "           lore   170   141   165    49    96   175 \n",
            "        mystery    42   141    13    57    30    20 \n",
            "           news    93    86    66    38    50   389 \n",
            "       religion    82    59    78    12    54    71 \n",
            "        reviews    45    40    45    26    19    58 \n",
            "        romance    74   193    11    51    45    43 \n",
            "science_fiction    16    49     4    12     8    16 \n"
          ]
        }
      ]
    }
  ]
}