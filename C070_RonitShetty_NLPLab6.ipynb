{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrRBXqQTiOjDbrSxthEebV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonitShetty/NLP-Labs/blob/main/C070_RonitShetty_NLPLab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Lab 6\n",
        "****\n",
        "**Aim:** Identify the Part of speech tagging in text data.\n",
        "\n",
        "a)\tPart of speech tagging - like noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection\n",
        "\n",
        "\n",
        "**Roll No.:** C070  \n",
        "**Name:** Ronit Shetty  \n",
        "**SAP ID:** 70322000128  \n",
        "**Division:** C  \n",
        "**Batch:** C1  "
      ],
      "metadata": {
        "id": "G02SSKK6pWSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"\\n--- Basic Token Analysis ---\")\n",
        "print(\"Used to demonstrate the fundamental token attributes that spaCy extracts from text\")\n",
        "doc = nlp(\"Natural Language Processing (NLP) enables machines to understand, interpret, and generate human language.\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:12} {token.lemma_:12} {token.pos_:6} {token.tag_:6} {token.dep_:10} {token.shape_:8} {token.is_alpha} {token.is_stop}\")\n",
        "\n",
        "displacy.render(doc, style = \"dep\", jupyter=True)\n",
        "\n",
        "\n",
        "# --- 1. Tokenization ---\n",
        "print(\"\\n--- 1. Tokenization ---\")\n",
        "print(\"Used to split raw text into meaningful units (tokens) like words, punctuation, and special characters\")\n",
        "text_tokenization = \"SpaCy's tokenization is powerful. U.K. is one token, and don't is split.\"\n",
        "doc_tokenization = nlp(text_tokenization)\n",
        "print(f\"Original Text: '{text_tokenization}'\")\n",
        "print(\"Tokens:\", [token.text for token in doc_tokenization])\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Sentence Segmentation ---\n",
        "print(\"\\n--- 2. Sentence Segmentation ---\")\n",
        "print(\"Used to identify sentence boundaries and split text into individual sentences\")\n",
        "text_segmentation = \"This is the first sentence. This is another one! And a final sentence?\"\n",
        "doc_segmentation = nlp(text_segmentation)\n",
        "for i, sent in enumerate(doc_segmentation.sents):\n",
        "    print(f\"Sentence {i+1}: '{sent.text}'\")\n",
        "\n",
        "# Check if sentence boundaries are available\n",
        "print(f\"Has sentence boundaries: {doc_segmentation.has_annotation('SENT_START')}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. POS Tagging & 4. Morphology ---\n",
        "print(\"\\n--- 3. POS Tagging & 4. Morphological Analysis ---\")\n",
        "print(\"Used to assign grammatical categories (noun, verb, etc.) and analyze word forms (tense, number, etc.)\")\n",
        "text_pos = \"She was running and eating cats quickly.\"\n",
        "doc_pos = nlp(text_pos)\n",
        "for token in doc_pos:\n",
        "    print(\n",
        "        f\"Token: {token.text:<12} | \"\n",
        "        f\"POS: {token.pos_:<8} | \"\n",
        "        f\"Tag: {token.tag_:<8} | \"\n",
        "        f\"Morph: {str(token.morph) if token.morph else 'None':<15} | \"\n",
        "        f\"Is_alpha: {token.is_alpha} | \"\n",
        "        f\"Is_stop: {token.is_stop}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# --- 5. Lemmatization ---\n",
        "print(\"\\n--- 5. Lemmatization ---\")\n",
        "print(\"Used to reduce words to their base or dictionary form (e.g., 'running' â†’ 'run')\")\n",
        "text_lemma = \"I am running in circles, seeing the best sights and geese.\"\n",
        "doc_lemma = nlp(text_lemma)\n",
        "for token in doc_lemma:\n",
        "    print(f\"Token: {token.text:<12} | Lemma: {token.lemma_:<12} | POS: {token.pos_}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 6. Dependency Parsing ---\n",
        "print(\"\\n--- 6. Dependency Parsing ---\")\n",
        "print(\"Used to analyze grammatical relationships between words and build syntactic trees\")\n",
        "text_dep = \"Apple is looking at buying a U.K. startup.\"\n",
        "doc_dep = nlp(text_dep)\n",
        "for token in doc_dep:\n",
        "    print(\n",
        "        f\"Token: {token.text:<12} | \"\n",
        "        f\"Dep: {token.dep_:<12} | \"\n",
        "        f\"Head: {token.head.text:<12} | \"\n",
        "        f\"Children: {[child.text for child in token.children]}\"\n",
        "    )\n",
        "\n",
        "# Check if dependency parsing is available\n",
        "print(f\"Has dependency parsing: {doc_dep.has_annotation('DEP')}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 7. Named Entity Recognition ---\n",
        "print(\"\\n--- 7. Named Entity Recognition (NER) ---\")\n",
        "print(\"Used to identify and classify named entities like people, organizations, locations, and dates\")\n",
        "text_ner = \"Apple Inc. is looking at buying a U.K. startup for $1 billion in London.\"\n",
        "doc_ner = nlp(text_ner)\n",
        "print(f\"Entities found: {len(doc_ner.ents)}\")\n",
        "for ent in doc_ner.ents:\n",
        "    print(\n",
        "        f\"Entity: {ent.text:<15} | \"\n",
        "        f\"Label: {ent.label_:<10} | \"\n",
        "        f\"Description: {spacy.explain(ent.label_)}\"\n",
        "    )\n",
        "\n",
        "# Token-level entity information\n",
        "print(\"\\nToken-level NER:\")\n",
        "for token in doc_ner:\n",
        "    if token.ent_type_:\n",
        "        print(f\"Token: {token.text:<12} | IOB: {token.ent_iob_:<3} | Type: {token.ent_type_}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 8. Noun Chunks ---\n",
        "print(\"\\n--- 8. Noun Chunks ---\")\n",
        "print(\"Used to extract base noun phrases that represent key concepts or entities in text\")\n",
        "text_chunks = \"The quick brown fox jumps over the lazy dog in the beautiful garden.\"\n",
        "doc_chunks = nlp(text_chunks)\n",
        "print(\"Noun chunks found:\")\n",
        "for chunk in doc_chunks.noun_chunks:\n",
        "    print(f\"Chunk: '{chunk.text}' | Root: '{chunk.root.text}' | Dep: {chunk.root.dep_}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 9. Word Vectors and Similarity (if available) ---\n",
        "print(\"\\n--- 9. Word Vectors and Similarity ---\")\n",
        "print(\"Used to measure semantic similarity between words, sentences, or documents using numerical representations\")\n",
        "# Check if the model has vectors\n",
        "if nlp.vocab.vectors.size:\n",
        "    doc1 = nlp(\"I like cats.\")\n",
        "    doc2 = nlp(\"I love dogs.\")\n",
        "    doc3 = nlp(\"I like pizza.\")\n",
        "\n",
        "    print(f\"Similarity between '{doc1.text}' and '{doc2.text}': {doc1.similarity(doc2):.3f}\")\n",
        "    print(f\"Similarity between '{doc1.text}' and '{doc3.text}': {doc1.similarity(doc3):.3f}\")\n",
        "\n",
        "    # Token vectors\n",
        "    cats_token = doc1[2]  # \"cats\"\n",
        "    print(f\"Vector for 'cats' has shape: {cats_token.vector.shape}\")\n",
        "    print(f\"Token 'cats' has vector: {cats_token.has_vector}\")\n",
        "else:\n",
        "    print(\"No word vectors available in this model (en_core_web_sm doesn't include vectors)\")\n",
        "    print(\"To get vectors, use a larger model like en_core_web_md or en_core_web_lg\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 10. Language Data (Stop words, etc.) ---\n",
        "print(\"\\n--- 10. Language Data ---\")\n",
        "print(\"Used to access language-specific information like stop words, punctuation rules, and linguistic patterns\")\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "print(f\"Number of stop words: {len(stop_words)}\")\n",
        "print(\"Sample stop words:\", list(stop_words)[:10])\n",
        "\n",
        "# Check specific words\n",
        "test_words = [\"the\", \"galaxy\", \"and\", \"python\"]\n",
        "for word in test_words:\n",
        "    is_stop = nlp.vocab[word].is_stop\n",
        "    print(f\"Is '{word}' a stop word? {is_stop}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 11. Token Shape and Character Analysis ---\n",
        "print(\"\\n--- 11. Token Shape and Character Analysis ---\")\n",
        "print(\"Used to analyze the orthographic patterns and character types in tokens (digits, punctuation, etc.)\")\n",
        "text_shape = \"The U.S.A. has 50 states and iPhone costs $999.99!\"\n",
        "doc_shape = nlp(text_shape)\n",
        "for token in doc_shape:\n",
        "    print(\n",
        "        f\"Token: {token.text:<12} | \"\n",
        "        f\"Shape: {token.shape_:<8} | \"\n",
        "        f\"Is_alpha: {token.is_alpha} | \"\n",
        "        f\"Is_digit: {token.is_digit} | \"\n",
        "        f\"Is_punct: {token.is_punct} | \"\n",
        "        f\"Like_num: {token.like_num} | \"\n",
        "        f\"Like_email: {token.like_email}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# --- 12. Retokenization (Merging and Splitting) ---\n",
        "print(\"\\n--- 12. Retokenization - Merging Tokens ---\")\n",
        "print(\"Used to modify tokenization by combining multiple tokens into one or splitting tokens apart\")\n",
        "doc_merge = nlp(\"I live in New York City and work there.\")\n",
        "print(\"Before merging:\", [token.text for token in doc_merge])\n",
        "\n",
        "# Merge \"New York City\" into a single token\n",
        "with doc_merge.retokenize() as retokenizer:\n",
        "    # Find the span for \"New York City\"\n",
        "    nyc_span = doc_merge[3:6]  # tokens 3, 4, 5\n",
        "    retokenizer.merge(nyc_span, attrs={\"LEMMA\": \"New York City\", \"ENT_TYPE\": \"GPE\"})\n",
        "\n",
        "print(\"After merging:\", [token.text for token in doc_merge])\n",
        "print(\"Lemma of merged token:\", doc_merge[3].lemma_)\n",
        "\n",
        "\n",
        "\n",
        "# --- 13. Splitting Tokens ---\n",
        "print(\"\\n--- 13. Retokenization - Splitting Tokens ---\")\n",
        "print(\"Used to split single tokens into multiple tokens when default tokenization is insufficient\")\n",
        "# Note: This is a complex operation, here's a simpler example\n",
        "doc_split = nlp(\"I'll go there.\")\n",
        "print(\"Before splitting:\", [token.text for token in doc_split])\n",
        "\n",
        "# For demonstration, let's show how splitting would work conceptually\n",
        "# (Actual splitting requires careful handling of heads and dependencies)\n",
        "print(\"Token 'I'll' could be split into: ['I', \\\"'ll\\\"]\")\n",
        "print(\"This requires specifying heads for proper dependency tree maintenance\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 14. Custom Attributes (Extension) ---\n",
        "print(\"\\n--- 14. Custom Attributes ---\")\n",
        "print(\"Used to add custom properties and methods to spaCy objects for domain-specific processing\")\n",
        "# Register a custom attribute\n",
        "from spacy.tokens import Token\n",
        "Token.set_extension(\"is_greeting\", default=False, force=True)\n",
        "\n",
        "doc_custom = nlp(\"Hello, how are you today?\")\n",
        "# Set custom attribute\n",
        "doc_custom[0]._.is_greeting = True\n",
        "\n",
        "for token in doc_custom:\n",
        "    print(f\"Token: {token.text:<8} | Is_greeting: {token._.is_greeting}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 15. Pipeline Components ---\n",
        "print(\"\\n--- 15. Pipeline Information ---\")\n",
        "print(\"Used to understand and manage the processing pipeline components and their order\")\n",
        "print(\"Pipeline components:\", nlp.pipe_names)\n",
        "print(\"Pipeline component details:\")\n",
        "for name, component in nlp.pipeline:\n",
        "    print(f\"  - {name}: {type(component).__name__}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 16. Span Analysis ---\n",
        "print(\"\\n--- 16. Span Analysis ---\")\n",
        "print(\"Used to work with continuous sequences of tokens as single units for analysis\")\n",
        "text_span = \"The European Union was established in 1993.\"\n",
        "doc_span = nlp(text_span)\n",
        "\n",
        "# Create a custom span\n",
        "eu_span = doc_span[1:3]  # \"European Union\"\n",
        "print(f\"Span text: '{eu_span.text}'\")\n",
        "print(f\"Span root: '{eu_span.root.text}'\")\n",
        "print(f\"Span label: {eu_span.label_}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 17. Lexical Attributes ---\n",
        "print(\"\\n--- 17. Lexical Attributes ---\")\n",
        "print(\"Used to access word-level properties like currency symbols, numbers, and vocabulary status\")\n",
        "text_lex = \"The price is twenty-five dollars and 50 cents.\"\n",
        "doc_lex = nlp(text_lex)\n",
        "for token in doc_lex:\n",
        "    print(\n",
        "        f\"Token: {token.text:<12} | \"\n",
        "        f\"Is_currency: {token.is_currency} | \"\n",
        "        f\"Like_num: {token.like_num} | \"\n",
        "        f\"Is_oov: {token.is_oov}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# --- 18. Statistical Information ---\n",
        "print(\"\\n--- 18. Model and Statistical Information ---\")\n",
        "print(\"Used to access metadata about the loaded model and vocabulary statistics\")\n",
        "print(f\"Model name: {nlp.meta.get('name', 'Unknown')}\")\n",
        "print(f\"Model version: {nlp.meta.get('version', 'Unknown')}\")\n",
        "print(f\"Model language: {nlp.meta.get('lang', 'Unknown')}\")\n",
        "print(f\"Vocabulary size: {len(nlp.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XRcVO2sPxWXB",
        "outputId": "907f7c34-6465-4e12-d06b-145cc13fc0b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Basic Token Analysis ---\n",
            "Used to demonstrate the fundamental token attributes that spaCy extracts from text\n",
            "Natural      Natural      PROPN  NNP    compound   Xxxxx    True False\n",
            "Language     Language     PROPN  NNP    compound   Xxxxx    True False\n",
            "Processing   Processing   PROPN  NNP    nsubj      Xxxxx    True False\n",
            "(            (            PUNCT  -LRB-  punct      (        False False\n",
            "NLP          NLP          PROPN  NNP    appos      XXX      True False\n",
            ")            )            PUNCT  -RRB-  punct      )        False False\n",
            "enables      enable       VERB   VBZ    ROOT       xxxx     True False\n",
            "machines     machine      NOUN   NNS    nsubj      xxxx     True False\n",
            "to           to           PART   TO     aux        xx       True True\n",
            "understand   understand   VERB   VB     ccomp      xxxx     True False\n",
            ",            ,            PUNCT  ,      punct      ,        False False\n",
            "interpret    interpret    VERB   VB     conj       xxxx     True False\n",
            ",            ,            PUNCT  ,      punct      ,        False False\n",
            "and          and          CCONJ  CC     cc         xxx      True True\n",
            "generate     generate     VERB   VB     conj       xxxx     True False\n",
            "human        human        ADJ    JJ     amod       xxxx     True False\n",
            "language     language     NOUN   NN     dobj       xxxx     True False\n",
            ".            .            PUNCT  .      punct      .        False False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5f0868513ea043158583d07a608eb326-0\" class=\"displacy\" width=\"2325\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Natural</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Language</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Processing (</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">NLP)</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">enables</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">machines</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">understand,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">interpret,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">generate</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">human</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">language.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-9\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1795.0,266.5 L1803.0,254.5 1787.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-10\" stroke-width=\"2px\" d=\"M1995,264.5 C1995,177.0 2140.0,177.0 2140.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1995,266.5 L1987,254.5 2003,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-5f0868513ea043158583d07a608eb326-0-11\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,89.5 2145.0,89.5 2145.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-5f0868513ea043158583d07a608eb326-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2145.0,266.5 L2153.0,254.5 2137.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. Tokenization ---\n",
            "Used to split raw text into meaningful units (tokens) like words, punctuation, and special characters\n",
            "Original Text: 'SpaCy's tokenization is powerful. U.K. is one token, and don't is split.'\n",
            "Tokens: ['SpaCy', \"'s\", 'tokenization', 'is', 'powerful', '.', 'U.K.', 'is', 'one', 'token', ',', 'and', 'do', \"n't\", 'is', 'split', '.']\n",
            "\n",
            "--- 2. Sentence Segmentation ---\n",
            "Used to identify sentence boundaries and split text into individual sentences\n",
            "Sentence 1: 'This is the first sentence.'\n",
            "Sentence 2: 'This is another one!'\n",
            "Sentence 3: 'And a final sentence?'\n",
            "Has sentence boundaries: True\n",
            "\n",
            "--- 3. POS Tagging & 4. Morphological Analysis ---\n",
            "Used to assign grammatical categories (noun, verb, etc.) and analyze word forms (tense, number, etc.)\n",
            "Token: She          | POS: PRON     | Tag: PRP      | Morph: Case=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs | Is_alpha: True | Is_stop: True\n",
            "Token: was          | POS: AUX      | Tag: VBD      | Morph: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin | Is_alpha: True | Is_stop: True\n",
            "Token: running      | POS: VERB     | Tag: VBG      | Morph: Aspect=Prog|Tense=Pres|VerbForm=Part | Is_alpha: True | Is_stop: False\n",
            "Token: and          | POS: CCONJ    | Tag: CC       | Morph: ConjType=Cmp    | Is_alpha: True | Is_stop: True\n",
            "Token: eating       | POS: VERB     | Tag: VBG      | Morph: Aspect=Prog|Tense=Pres|VerbForm=Part | Is_alpha: True | Is_stop: False\n",
            "Token: cats         | POS: NOUN     | Tag: NNS      | Morph: Number=Plur     | Is_alpha: True | Is_stop: False\n",
            "Token: quickly      | POS: ADV      | Tag: RB       | Morph: None            | Is_alpha: True | Is_stop: False\n",
            "Token: .            | POS: PUNCT    | Tag: .        | Morph: PunctType=Peri  | Is_alpha: False | Is_stop: False\n",
            "\n",
            "--- 5. Lemmatization ---\n",
            "Used to reduce words to their base or dictionary form (e.g., 'running' â†’ 'run')\n",
            "Token: I            | Lemma: I            | POS: PRON\n",
            "Token: am           | Lemma: be           | POS: AUX\n",
            "Token: running      | Lemma: run          | POS: VERB\n",
            "Token: in           | Lemma: in           | POS: ADP\n",
            "Token: circles      | Lemma: circle       | POS: NOUN\n",
            "Token: ,            | Lemma: ,            | POS: PUNCT\n",
            "Token: seeing       | Lemma: see          | POS: VERB\n",
            "Token: the          | Lemma: the          | POS: DET\n",
            "Token: best         | Lemma: good         | POS: ADJ\n",
            "Token: sights       | Lemma: sight        | POS: NOUN\n",
            "Token: and          | Lemma: and          | POS: CCONJ\n",
            "Token: geese        | Lemma: goose        | POS: NOUN\n",
            "Token: .            | Lemma: .            | POS: PUNCT\n",
            "\n",
            "--- 6. Dependency Parsing ---\n",
            "Used to analyze grammatical relationships between words and build syntactic trees\n",
            "Token: Apple        | Dep: nsubj        | Head: looking      | Children: []\n",
            "Token: is           | Dep: aux          | Head: looking      | Children: []\n",
            "Token: looking      | Dep: ROOT         | Head: looking      | Children: ['Apple', 'is', 'at', '.']\n",
            "Token: at           | Dep: prep         | Head: looking      | Children: ['buying']\n",
            "Token: buying       | Dep: pcomp        | Head: at           | Children: ['startup']\n",
            "Token: a            | Dep: det          | Head: startup      | Children: []\n",
            "Token: U.K.         | Dep: compound     | Head: startup      | Children: []\n",
            "Token: startup      | Dep: dobj         | Head: buying       | Children: ['a', 'U.K.']\n",
            "Token: .            | Dep: punct        | Head: looking      | Children: []\n",
            "Has dependency parsing: True\n",
            "\n",
            "--- 7. Named Entity Recognition (NER) ---\n",
            "Used to identify and classify named entities like people, organizations, locations, and dates\n",
            "Entities found: 4\n",
            "Entity: Apple Inc.      | Label: ORG        | Description: Companies, agencies, institutions, etc.\n",
            "Entity: U.K.            | Label: GPE        | Description: Countries, cities, states\n",
            "Entity: $1 billion      | Label: MONEY      | Description: Monetary values, including unit\n",
            "Entity: London          | Label: GPE        | Description: Countries, cities, states\n",
            "\n",
            "Token-level NER:\n",
            "Token: Apple        | IOB: B   | Type: ORG\n",
            "Token: Inc.         | IOB: I   | Type: ORG\n",
            "Token: U.K.         | IOB: B   | Type: GPE\n",
            "Token: $            | IOB: B   | Type: MONEY\n",
            "Token: 1            | IOB: I   | Type: MONEY\n",
            "Token: billion      | IOB: I   | Type: MONEY\n",
            "Token: London       | IOB: B   | Type: GPE\n",
            "\n",
            "--- 8. Noun Chunks ---\n",
            "Used to extract base noun phrases that represent key concepts or entities in text\n",
            "Noun chunks found:\n",
            "Chunk: 'The quick brown fox' | Root: 'fox' | Dep: nsubj\n",
            "Chunk: 'the lazy dog' | Root: 'dog' | Dep: pobj\n",
            "Chunk: 'the beautiful garden' | Root: 'garden' | Dep: pobj\n",
            "\n",
            "--- 9. Word Vectors and Similarity ---\n",
            "Used to measure semantic similarity between words, sentences, or documents using numerical representations\n",
            "No word vectors available in this model (en_core_web_sm doesn't include vectors)\n",
            "To get vectors, use a larger model like en_core_web_md or en_core_web_lg\n",
            "\n",
            "--- 10. Language Data ---\n",
            "Used to access language-specific information like stop words, punctuation rules, and linguistic patterns\n",
            "Number of stop words: 326\n",
            "Sample stop words: ['am', 'â€˜m', 'because', 'but', 'sometime', 'else', 'by', 'become', 'none', 'than']\n",
            "Is 'the' a stop word? True\n",
            "Is 'galaxy' a stop word? False\n",
            "Is 'and' a stop word? True\n",
            "Is 'python' a stop word? False\n",
            "\n",
            "--- 11. Token Shape and Character Analysis ---\n",
            "Used to analyze the orthographic patterns and character types in tokens (digits, punctuation, etc.)\n",
            "Token: The          | Shape: Xxx      | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: U.S.A.       | Shape: X.X.X.   | Is_alpha: False | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: has          | Shape: xxx      | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: 50           | Shape: dd       | Is_alpha: False | Is_digit: True | Is_punct: False | Like_num: True | Like_email: False\n",
            "Token: states       | Shape: xxxx     | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: and          | Shape: xxx      | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: iPhone       | Shape: xXxxxx   | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: costs        | Shape: xxxx     | Is_alpha: True | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: $            | Shape: $        | Is_alpha: False | Is_digit: False | Is_punct: False | Like_num: False | Like_email: False\n",
            "Token: 999.99       | Shape: ddd.dd   | Is_alpha: False | Is_digit: False | Is_punct: False | Like_num: True | Like_email: False\n",
            "Token: !            | Shape: !        | Is_alpha: False | Is_digit: False | Is_punct: True | Like_num: False | Like_email: False\n",
            "\n",
            "--- 12. Retokenization - Merging Tokens ---\n",
            "Used to modify tokenization by combining multiple tokens into one or splitting tokens apart\n",
            "Before merging: ['I', 'live', 'in', 'New', 'York', 'City', 'and', 'work', 'there', '.']\n",
            "After merging: ['I', 'live', 'in', 'New York City', 'and', 'work', 'there', '.']\n",
            "Lemma of merged token: New York City\n",
            "\n",
            "--- 13. Retokenization - Splitting Tokens ---\n",
            "Used to split single tokens into multiple tokens when default tokenization is insufficient\n",
            "Before splitting: ['I', \"'ll\", 'go', 'there', '.']\n",
            "Token 'I'll' could be split into: ['I', \"'ll\"]\n",
            "This requires specifying heads for proper dependency tree maintenance\n",
            "\n",
            "--- 14. Custom Attributes ---\n",
            "Used to add custom properties and methods to spaCy objects for domain-specific processing\n",
            "Token: Hello    | Is_greeting: True\n",
            "Token: ,        | Is_greeting: False\n",
            "Token: how      | Is_greeting: False\n",
            "Token: are      | Is_greeting: False\n",
            "Token: you      | Is_greeting: False\n",
            "Token: today    | Is_greeting: False\n",
            "Token: ?        | Is_greeting: False\n",
            "\n",
            "--- 15. Pipeline Information ---\n",
            "Used to understand and manage the processing pipeline components and their order\n",
            "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "Pipeline component details:\n",
            "  - tok2vec: Tok2Vec\n",
            "  - tagger: Tagger\n",
            "  - parser: DependencyParser\n",
            "  - attribute_ruler: AttributeRuler\n",
            "  - lemmatizer: EnglishLemmatizer\n",
            "  - ner: EntityRecognizer\n",
            "\n",
            "--- 16. Span Analysis ---\n",
            "Used to work with continuous sequences of tokens as single units for analysis\n",
            "Span text: 'European Union'\n",
            "Span root: 'Union'\n",
            "Span label: \n",
            "\n",
            "--- 17. Lexical Attributes ---\n",
            "Used to access word-level properties like currency symbols, numbers, and vocabulary status\n",
            "Token: The          | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: price        | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: is           | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: twenty       | Is_currency: False | Like_num: True | Is_oov: True\n",
            "Token: -            | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: five         | Is_currency: False | Like_num: True | Is_oov: True\n",
            "Token: dollars      | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: and          | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: 50           | Is_currency: False | Like_num: True | Is_oov: True\n",
            "Token: cents        | Is_currency: False | Like_num: False | Is_oov: True\n",
            "Token: .            | Is_currency: False | Like_num: False | Is_oov: True\n",
            "\n",
            "--- 18. Model and Statistical Information ---\n",
            "Used to access metadata about the loaded model and vocabulary statistics\n",
            "Model name: core_web_sm\n",
            "Model version: 3.8.0\n",
            "Model language: en\n",
            "Vocabulary size: 852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question of curiosity:**\n",
        "\n",
        "1.\tHow HMM can be used in POS tagging?\n",
        "\n",
        "Ans: A Hidden Markov Model (HMM) is a statistical model perfectly suited for sequence-labeling tasks like POS tagging because it excels at finding the most probable sequence of tags for a given sentence. It works by modeling two key probabilities. First, emission probabilities represent the likelihood of a specific word appearing given a certain POS tag (e.g., the word \"run\" is highly likely to be a VERB). Second, transition probabilities represent the likelihood of one POS tag following another (e.g., a NOUN is very likely to follow a DETERMINER like \"the\").\n",
        "\n",
        "The POS tags themselves are considered the \"hidden states\" that we can't see, while the words are the \"observations\" that are visible. Using these probabilities, an efficient algorithm called the Viterbi algorithm calculates the single most likely sequence of hidden tags that could have produced the observed sentence, effectively assigning a POS tag to every word.\n",
        "\n",
        "2.\tWhat are the Applications of NER?\n",
        "\n",
        "â€¢\tQuestion Answering Systems: NER can be used to help question-answering systems identify the entities mentioned in a question and retrieve the relevant information from a knowledge base.\n",
        "\n",
        "Ans: Named Entity Recognition (NER) has a vast range of real-world applications focused on extracting structured information from unstructured text. In customer support, NER can automatically identify product names or order numbers in a complaint to route it to the right department. HR departments use it to parse resumes, instantly pulling out names, skills, and past employers to populate databases. In the media, NER helps in content classification and recommendation by identifying the people, organizations, and locations mentioned in articles to suggest similar content to readers. It's also the backbone of many question-answering systems like Siri and Alexa, enabling them to understand what you're asking about. Furthermore, in fields like healthcare, NER is critical for extracting patient data, diseases, and medication names from doctors' notes, while in finance, it can be used to monitor news for mentions of specific companies for market analysis.\n"
      ],
      "metadata": {
        "id": "7pV58Slh5Yjc"
      }
    }
  ]
}