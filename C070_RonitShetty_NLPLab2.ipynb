{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1Ca747Qq/EvYP8Ld1W6A7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonitShetty/NLP-Labs/blob/main/C070_RonitShetty_NLPLab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SOGgCU0saKaF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Label Encoding**"
      ],
      "metadata": {
        "id": "OaiXRP1ia9rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Label Encoding ---\n",
        "encoder = LabelEncoder()\n",
        "labels = [\"positive\",\"negative\",\"neutral\",\"positive\"]\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "encoded_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COzVCAFwaoOu",
        "outputId": "0420396b-4942-44f3-c839-a77f52643d61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**One Hot Encoding**"
      ],
      "metadata": {
        "id": "HwxK15UibFxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Ordinal Encoding ---\n",
        "data = np.array(labels).reshape(-1,1)\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "onehot_labels = onehot_encoder.fit_transform(data)\n",
        "onehot_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FiF_t2QbL-N",
        "outputId": "03138ecc-7642-464f-826d-aa47cbbc5230"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**BOW and TF-IDF on kaggle dataset (Tweets.csv)**"
      ],
      "metadata": {
        "id": "A0tyO77YiehV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Import Necessary Libraries ---\n",
        "# pandas is used for data manipulation and reading CSV files.\n",
        "# nltk (Natural Language Toolkit) is used for text processing tasks like tokenization and stop word removal.\n",
        "# scikit-learn (sklearn) is used for machine learning, from splitting data to building and evaluating models.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- Step 1: Load and Prepare the Dataset ---\n",
        "# We start by loading the 'Tweets.csv' file into a pandas DataFrame.\n",
        "# A try-except block is used to handle the case where the file might not be found.\n",
        "# We then select only the 'text' and 'airline_sentiment' columns, as they are the ones we need.\n",
        "# Finally, `dropna()` removes any rows with missing values to ensure data quality.\n",
        "\n",
        "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
        "try:\n",
        "    df = pd.read_csv('Tweets.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Tweets.csv' not found. Using a sample DataFrame for demonstration.\")\n",
        "    df = pd.DataFrame({\n",
        "        'text': [\"Sample positive tweet\", \"Sample negative tweet\", \"This is a neutral one.\"],\n",
        "        'airline_sentiment': [\"positive\", \"negative\", \"neutral\"]\n",
        "    })\n",
        "\n",
        "df = df[['text', 'airline_sentiment']]\n",
        "df.dropna(inplace=True)\n",
        "print(\"Data loaded successfully.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 2: Preprocess the Text Data ---\n",
        "# This is a crucial step to clean and standardize the text data.\n",
        "# 1. Tokenization: We break down each tweet into individual words (tokens).\n",
        "# 2. Lowercasing: All words are converted to lowercase to treat words like \"Flight\" and \"flight\" as the same.\n",
        "# 3. Stop Word Removal: Common English words that don't add much meaning (e.g., 'a', 'the', 'is') are removed.\n",
        "# 4. Stemming: Words are reduced to their root form (e.g., 'flying', 'fly' -> 'fli'). This helps group related words.\n",
        "# The `preprocess_text` function performs these steps, and we apply it to every tweet in our DataFrame.\n",
        "\n",
        "print(\"--- Step 2: Preprocessing Text ---\")\n",
        "# FIX: Explicitly download the necessary NLTK data. This resolves the LookupError.\n",
        "# The 'punkt' package is for tokenization and 'stopwords' is for the list of stop words.\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Keep only alphabetic tokens and remove stop words before stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "print(\"Text preprocessing complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 3: Inspect the Preprocessed Output (Optional but Recommended) ---\n",
        "# Before moving on, it's a good practice to look at the results of our preprocessing.\n",
        "# This helps verify that our cleaning function is working as expected. We'll look at the\n",
        "# original vs. processed text and the list of tokens for the first tweet.\n",
        "\n",
        "print(\"--- Step 3: Inspecting Processed Text and Tokens ---\")\n",
        "# Print the original vs. processed text for the first 5 tweets\n",
        "print(\"Original vs. Processed Text:\")\n",
        "print(df[['text', 'processed_text']].head())\n",
        "print(\"\\nTokens from the first processed tweet:\")\n",
        "# Get the first processed tweet and split it to see the list of tokens\n",
        "first_processed_tweet = df['processed_text'].iloc[0]\n",
        "tokens = first_processed_tweet.split()\n",
        "print(tokens)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 4: Split Data into Training and Testing Sets ---\n",
        "# We split our dataset to train the model on one part (training set) and evaluate its\n",
        "# performance on a separate, unseen part (testing set). This shows how well the model generalizes.\n",
        "# `test_size=0.2` means 20% of the data is for testing.\n",
        "# `stratify=y` ensures that the proportion of sentiments (positive, negative, neutral) is the same\n",
        "# in both the training and testing sets, which is important for imbalanced datasets.\n",
        "\n",
        "print(\"--- Step 4: Splitting Data ---\")\n",
        "X = df['processed_text']  # Features (the processed text)\n",
        "y = df['airline_sentiment'] # Target (the sentiment labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Training set size: {len(X_train)}, Testing set size: {len(X_test)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 5: Encode Target Labels ---\n",
        "# Machine learning models require numerical inputs. Our sentiment labels ('positive', 'negative', 'neutral')\n",
        "# are strings. `LabelEncoder` converts these strings into numbers (e.g., 0, 1, 2).\n",
        "# We `fit_transform` on the training labels and only `transform` the test labels to prevent data leakage.\n",
        "\n",
        "print(\"--- Step 5: Applying Label Encoding to Target Variable ---\")\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Print the mapping to see which number corresponds to which sentiment\n",
        "print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 6: Vectorize Text Data (BoW and TF-IDF) ---\n",
        "# Here, we convert the cleaned text into numerical vectors that our model can understand.\n",
        "# We will create two separate sets of features to compare their performance.\n",
        "\n",
        "print(\"--- Step 6: Vectorizing Text ---\")\n",
        "# Method A: Bag-of-Words (BoW)\n",
        "# BoW represents text by counting the frequency of each word. It's a simple and effective method.\n",
        "# `max_features=2000` limits the vocabulary to the 2000 most frequent words.\n",
        "bow_vectorizer = CountVectorizer(max_features=2000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "print(\"BoW vectorization complete.\")\n",
        "\n",
        "# Method B: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# TF-IDF scores words based on their importance. It gives higher weight to words that are frequent\n",
        "# in a specific document but rare across all documents. This often performs better than BoW.\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "print(\"TF-IDF vectorization complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 7: Inspect the Vocabulary ---\n",
        "# Let's look at the words the BoW vectorizer learned from the training data. This is our model's \"vocabulary\".\n",
        "# The length should match the `max_features` we set earlier.\n",
        "\n",
        "print(\"--- Step 7: Inspecting BoW Vocabulary ---\")\n",
        "bow_vocabulary = bow_vectorizer.get_feature_names_out()\n",
        "print(\"Sample of BoW Vocabulary (first 100 words):\")\n",
        "print(list(bow_vocabulary[:100]))\n",
        "print(f\"\\nTotal vocabulary size: {len(bow_vocabulary)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 8: Model Training and Evaluation ---\n",
        "# Now we train our classification model (Logistic Regression) on the vectorized data and evaluate its performance.\n",
        "# We do this separately for both BoW and TF-IDF features to see which representation works better.\n",
        "\n",
        "print(\"--- Step 8: Training and Evaluating Models ---\")\n",
        "# Train and evaluate on Bag-of-Words (BoW) features\n",
        "print(\"\\n--- Model Training with BoW ---\")\n",
        "model_bow = LogisticRegression(max_iter=1000) # max_iter increased for convergence\n",
        "model_bow.fit(X_train_bow, y_train_encoded) # Train the model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bow = model_bow.predict(X_test_bow)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_bow = accuracy_score(y_test_encoded, y_pred_bow)\n",
        "print(f\"Accuracy with BoW features: {accuracy_bow:.4f}\")\n",
        "print(\"Classification Report (BoW):\\n\", classification_report(y_test_encoded, y_pred_bow, target_names=label_encoder.classes_))\n",
        "\n",
        "\n",
        "# Train and evaluate on TF-IDF features\n",
        "print(\"\\n--- Model Training with TF-IDF ---\")\n",
        "model_tfidf = LogisticRegression(max_iter=1000)\n",
        "model_tfidf.fit(X_train_tfidf, y_train_encoded) # Train the model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_tfidf = accuracy_score(y_test_encoded, y_pred_tfidf)\n",
        "print(f\"Accuracy with TF-IDF features: {accuracy_tfidf:.4f}\")\n",
        "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_test_encoded, y_pred_tfidf, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "jjkLX4TNsJA4",
        "outputId": "c4c8767f-dc79-41bd-9310-a414a5b8d93c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Preparing Data ---\n",
            "Data loaded successfully.\n",
            "--------------------------------------------------\n",
            "--- Step 2: Preprocessing Text ---\n",
            "Text preprocessing complete.\n",
            "--------------------------------------------------\n",
            "--- Step 3: Inspecting Processed Text and Tokens ---\n",
            "Original vs. Processed Text:\n",
            "                                                text  \\\n",
            "0                @VirginAmerica What @dhepburn said.   \n",
            "1  @VirginAmerica plus you've added commercials t...   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...   \n",
            "3  @VirginAmerica it's really aggressive to blast...   \n",
            "4  @VirginAmerica and it's a really big bad thing...   \n",
            "\n",
            "                                      processed_text  \n",
            "0                        virginamerica dhepburn said  \n",
            "1         virginamerica plu ad commerci experi tacki  \n",
            "2  virginamerica today must mean need take anoth ...  \n",
            "3  virginamerica realli aggress blast obnoxi ente...  \n",
            "4                 virginamerica realli big bad thing  \n",
            "\n",
            "Tokens from the first processed tweet:\n",
            "['virginamerica', 'dhepburn', 'said']\n",
            "--------------------------------------------------\n",
            "--- Step 4: Splitting Data ---\n",
            "Training set size: 11712, Testing set size: 2928\n",
            "--------------------------------------------------\n",
            "--- Step 5: Applying Label Encoding to Target Variable ---\n",
            "Label mapping: {'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}\n",
            "--------------------------------------------------\n",
            "--- Step 6: Vectorizing Text ---\n",
            "BoW vectorization complete.\n",
            "TF-IDF vectorization complete.\n",
            "--------------------------------------------------\n",
            "--- Step 7: Inspecting BoW Vocabulary ---\n",
            "Sample of BoW Vocabulary (first 100 words):\n",
            "['aa', 'aadvantag', 'abc', 'abil', 'abl', 'abq', 'absolut', 'absurd', 'abt', 'abysm', 'accept', 'access', 'accommod', 'accord', 'account', 'acct', 'accur', 'acknowledg', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'addit', 'address', 'admir', 'advanc', 'advantag', 'advertis', 'advic', 'advis', 'advisori', 'affect', 'afford', 'afternoon', 'age', 'agenc', 'agent', 'ago', 'agre', 'ah', 'ahead', 'ahold', 'air', 'airbu', 'aircanada', 'aircraft', 'airfar', 'airlin', 'airplan', 'airport', 'airway', 'aisl', 'alaska', 'alert', 'allianc', 'allow', 'almost', 'alon', 'along', 'alot', 'alreadi', 'alright', 'also', 'altern', 'although', 'alway', 'amaz', 'america', 'american', 'americanair', 'americanairlin', 'among', 'amount', 'amp', 'angri', 'anniversari', 'announc', 'annoy', 'anoth', 'answer', 'anymor', 'anyon', 'anyth', 'anytim', 'anyway', 'anywher', 'apart', 'apolog', 'app', 'appar', 'appear', 'appeas', 'appl', 'appli', 'applic', 'appreci', 'approach']\n",
            "\n",
            "Total vocabulary size: 2000\n",
            "--------------------------------------------------\n",
            "--- Step 8: Training and Evaluating Models ---\n",
            "\n",
            "--- Model Training with BoW ---\n",
            "Accuracy with BoW features: 0.7869\n",
            "Classification Report (BoW):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.89      0.87      1835\n",
            "     neutral       0.63      0.59      0.61       620\n",
            "    positive       0.74      0.64      0.69       473\n",
            "\n",
            "    accuracy                           0.79      2928\n",
            "   macro avg       0.74      0.71      0.72      2928\n",
            "weighted avg       0.78      0.79      0.78      2928\n",
            "\n",
            "\n",
            "--- Model Training with TF-IDF ---\n",
            "Accuracy with TF-IDF features: 0.7903\n",
            "Classification Report (TF-IDF):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.93      0.87      1835\n",
            "     neutral       0.66      0.53      0.59       620\n",
            "    positive       0.79      0.58      0.67       473\n",
            "\n",
            "    accuracy                           0.79      2928\n",
            "   macro avg       0.76      0.68      0.71      2928\n",
            "weighted avg       0.78      0.79      0.78      2928\n",
            "\n"
          ]
        }
      ]
    }
  ]
}